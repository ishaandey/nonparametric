# (PART) Two Sample Inference {-}


# Introduction {-}

> Goal: We've got two samples, and want to see if there's the significance difference in means.

## Background {-}

Comparing differences in means is one of the most commonly used procedures in statistics. Take the following use case: Does Product A have better ratings than than Product B? Sure, we could compare averages (i.e. Product A's mean rating is $4.83$, better than Product B's  $4.79$), but it fails to answer the question: Are the *population* ratings for A better than B? We only have data from a sample of reviews, so we've got to somehow *estimate* the population differences in ratings.

Any student of introductory statistics knows the remedy: take into account the spread of the data using a two-sample t-test. But the t-test makes a few strong assumptions about the data, mainly the assumption of normality. This chapter describes alternative tests we can use instead. 

## Notation {-}

Say you're expecting population 1 to be *lower* than population 2. You'll use a *lower-tail* test, and use a set of hypotheses like so: 
$$
\begin{aligned}
\text{Null Hypothesis }H_0 &: \mu_1 = \mu_2 \\
\text{Alternate Hypothesis }H_a &: \mu_1 < \mu_2
\end{aligned}
$$

In this section, we don't necessarily know the distribution of our data, but we're testing that the *distributions*, not necessarily means, are equal.

As such, our hypotheses will look something like:

$$
H_0: F_1(x) = F_2(x) \\
H_a: F_1(x) \geq F_2(x) \\
\text{ with a strict inequality for at least one }x
$$

$F(x)$ here refers to the empirical CDF, or the *probability of seeing a value less than or below* $x$.

If the location of population 1 is below population 2, we'd expect that at a certain point $x$, the probability of seeing a value at or below $x$ should be higher for population 1 than  it is for population 2. Here's a visual:

```{r, fig.align='center', eval=T, echo=F}
library(ggplot2)

x <- seq(-4, 4, length=100)
pdf.1 <- dnorm(x,0,1)
pdf.2 <- dnorm(x,1,1)

cdf.1 <- pnorm(x,0,1)
cdf.2 <- pnorm(x,1,1)

df <- data.frame(x, pdf.1, pdf.2, cdf.1, cdf.2)

pdfs <- ggplot(data=df, aes(x=x)) + 
  geom_line(aes(y = pdf.1, color = "Mean = 0"), size=1) + 
  geom_line(aes(y = pdf.2, color = "Mean = 1"), size=1) +
   theme(axis.title.x = element_blank(),
        panel.grid.minor = element_blank(),
        plot.title = element_text(hjust = 0.5),
        legend.position = c(0.15,0.75)) +
  ylab('Probability Density') + ggtitle('Comparing Distributions') +
  scale_color_manual(values = c("Mean = 0" = col1, "Mean = 1" = col2)) +
  labs(colour = "Location Parameter") +
  scale_y_continuous(breaks=c(0))

  
cdfs <- ggplot(data=df, aes(x=x)) + 
  geom_line(aes(y = cdf.1), color = col1, size=1) + 
  geom_line(aes(y = cdf.2), color = col2, size=1) +
  theme(panel.grid.minor = element_blank()) + 
  ylab('Cumulative Probability') +
  scale_y_continuous(breaks=c(0,1))
  
gridExtra::grid.arrange(pdfs,cdfs,ncol=1)
```

From the first plot we see $\mu_1 < \mu_2$, and from the second, $F_1(x) \geq F_2(x)$. Both correspond to the "less than" hypothesis.

## Diagnostics {-}

A big assumption parametric tests take is that the data is normally distributed. There's a few approaches we can take to validate this.

First is the QQ plot:

We could also use a more rigorous test of normality:

But its almost always just worth checking the distribution regardless: 


# Choosing a Test {-}

Are your population distributions normal, or, are there more than 40 observations in each sample? If so, choose the **Two Sample t-Test**. If not, read on:

Do you care about the magnitude of the difference? Consider the **Permutation Test**. Check the distribution and number of outliers. If the data is skewed, use the *median* variant, and if there are outliers on both tails, consider using trimmed means. 

If you're instead just looking to determine if one sample is greater than the other, use the **Wilcoxon Rank-Sum**, particularly with skewed distributions or heavy outliers. If we're interested in generating a confidence interval for the difference, we can use the **Mann-Whitney** test. It works similarly to the Wilcoxon Rank-Sum, and results in the same p-value.

## Power {-}

Include a discussion about power here


# Two Sample t-Test {#two-sample-t-test}

The two-sample t-test is by far the most powerful test when the assumptions are met for comparing means between two samples. **Assumptions**:

1. Random sample from each population
2. Both samples are independent
3. Both population distributions are normal
4. Both population variances are equal

Note: By the Central Limit Theorem, we can assume that the sample means will start looking normal at large sample sizes ($n \geq 40$).

## How It Works {-}

Do we know the population variance $\sigma^2$? 
- If so, we'll use the $z$ distribution: $z\sim N(0,1)$
- Otherwise, we'll use the $t$ distribution, $t\sim t(df)$, where $df$ is the minimum of the two sample sizes - 1.

## Code {-}

::: {.tab}
<button class="tablinks" onclick="unrolltab(event, 'R')">R</button>
<button class="tablinks" onclick="unrolltab(event, 'Python')">Python</button>

::: {#R .tabcontent}

```{r}
library(stats)

sample1 <- c(1.1, 2.1, 4.2, 3.2, 1.7, 2.2, 2.7)
sample2 <- c(3.9, 2.9, 3.8, 1.8, 3.3, 2.8, 2.3)

t.test(x=sample1, y=sample2, alternative='two.sided', paired=F)
```

:::

::: {#Python .tabcontent}

```{r engine='python'}
from scipi.stats import ttest_ind

sample1 = [1.1, 2.1, 4.2, 3.2, 1.7, 2.2, 2.7]
sample2 = [3.9, 2.9, 3.8, 1.8, 3.3, 2.8, 2.3]

ttest_ind(a=sample1, b=sample2)
```

:::
:::
<script> document.getElementsByClassName('tablinks')[0].click() </script>



# Permutation Test {#two-sample-permutation}

Use the permutation test if the normality assumption is violated, and you're interested in quantifying the difference in some location parameter: mean, trimmed mean, or median. This works well for smaller sample sizes. 

**Assumptions**: 

1. Random sample from each population
2. Both are sampled independently
3. Both population distributions are *continuous* (not categorical / discrete)

Note: We no longer need the assumption of normality, nor equal variances

## How It Works {-}

We'll use $D_{obs}$ to represent the difference in means that we observe between our samples. For the following samples, we find $D_{obs}=12.5$.

```{r, echo=F, eval=T, results=T}
sample2 <- c(46, 48, 49, 51)
sample1 <- c(31, 32, 34, 47)

df <- tibble('Sample A'=c(sample1, paste('Mean:',mean(sample1))), 
             'Sample B'=c(sample2, paste('Mean:',mean(sample2)))) %>% t()
kbl(df) %>% 
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(5, border_right = T) %>%
  row_spec(1, color=col1) %>%
  row_spec(2, color=col3) %>%
  kable_styling(full_width = F, html_font = '"Karla", calibri, sans-serif')
```

Under the null hypothesis, we'd expect that there is no difference in means. In other words, we could randomly switch around the values across the samples and generate test statistics $D^*$ for each permutation. If the null hypothesis is false, and the difference we observe can't be explained by random chance, we'd see that only a few $D^*$'s are more extreme than $D_{obs}$.

The way we do this is fairly straightforward. We'll first pool together our observed values.

```{r, echo=F, eval=T, results=T}
pooled <- c(sample1, sample2)

df = data.frame('Pooled'=pooled)
kbl(t(df), escape=F, align='c') %>% 
  kable_styling(full_width = F, position='center', 
                html_font = '"Karla", calibri, sans-serif') %>%
  column_spec(1, bold = T, border_right = T)
```

Now we'll create as many permutations as we can, reassigning the "Sample A" and "Sample B   " labels across all the observations. There are $\binom {m+n}{m} = \binom {8}{4} = 70$ possible permutations. We'll calculate a distribution of test statistics by finding what $D^*$ is for each permutation:

```{r,  echo=F, eval=T, results=T}
sample1 <- c(31, 32, 34, 47)
sample2 <- c(46, 48, 49, 51)

sample1.1 <- c(46, 32, 34, 47)
sample2.1 <- c(31, 48, 49, 51)

sample1.2 <- c(46, 48, 34, 47)
sample2.2 <- c(31, 32, 49, 51)

sample1.3 <- c(47, 48, 49, 51)
sample2.3 <- c(31, 32, 34, 46) 

sample.x <- rep('...', 4)

df <- tibble('A*'=sample1.1, 'B*'=sample2.1,
             'A**'=sample1.2, 'B**'=sample2.2,
             'etc'=sample.x, 
             'A***'=sample1.3, 'B***'=sample2.3,
             )
row.names(df) <- NULL

kbl(df, escape=F, align='c') %>% 
  kable_styling(full_width = F, position='center', 
                html_font = '"Karla", calibri, sans-serif') %>%
  column_spec(c(2,4,5,7), border_right = T) %>%
  column_spec(c(2,4,7), color = col3) %>%
  column_spec(c(1,3,6), color = col1, border_right = F) %>%
  column_spec(1, border_left=T) %>%
  add_header_above(c("Permutation 1" = 2, "Permutation 2" = 2, "..." = 1, "Permutation 70" = 2))
```


```{r, echo=F, eval=T, results=T}
d.1 <- mean(sample1.1) - mean(sample2.1)
d.2 <- mean(sample1.2) - mean(sample2.2)
d.3 <- mean(sample1.3) - mean(sample2.3)

df2 <- tibble('D*'  =paste('D*=',d.1),
              'D**' =paste('D*=',d.2),
              'etc' ='...', 
              'D***'=paste('D*=',d.3))
row.names(df2) <- NULL
colnames(df2) <- NULL
kbl(df2, escape=F, align='c') %>% 
  kable_styling(full_width = F, position='center', 
                html_font = '"Karla", calibri, sans-serif') %>%
  column_spec(1:4, border_right = T, border_left = T) %>%
  add_header_above(c("Permutation 1 " = 1, "Permutation 2" = 1, "..." = 1, " Permutation 70" = 1))
```

From our calculated $D^*$'s, we find that there are only 4 permutations that yield a test statistic greater than $D_{obs}$. So, our p-value is simply $\frac{4}{70}= 0.057$. 

<details><summary> Formal Definitions </summary>
<p>

For a double sided test:
$$
H_0: F_1(x) = F_2(x) \\
H_a: F_1(x) \neq  F_2(x) \\
~ \\
p\text{-value}_{two\ sided} = \frac{\text{# of |D's|}~\geq~|D_{obs}|}{\binom {m+n}{m}}
$$
For an upper tail test:
$$
H_0: F_1(x) = F_2(x) \\
H_a: F_1(x) \leq  F_2(x) \\
~ \\
p\text{-value}_{upper} = \frac{\text{# of }D\geq D_{obs}}{\binom {m+n}{m}}
$$

For a lower tail test:
$$
H_0: F_1(x) = F_2(x) \\
H_a: F_1(x) \geq  F_2(x) \\
~ \\
p\text{-value}_{lower} = \frac{\text{# of }D\leq D_{obs}}{\binom {m+n}{m}}
$$

</p>
</details>


**Interpretation**: Given a p-value of 0.057, there is a 5.7% chance of observing a difference as extreme as we did under the hypothesis that these samples come from populations with the same distribution.

## Code {-}
::: {.tab}
<button class="tablinks" onclick="unrolltab(event, 'R')">R</button>
<button class="tablinks" onclick="unrolltab(event, 'Python')">Python</button>

::: {#R .tabcontent}

```{r}
source('https://raw.githubusercontent.com/ishaandey/nonparametric/master/helper.R')

sample1 <- c(46, 48, 49, 51)
sample2 <- c(31, 32, 34, 47)

permutation_test(sample1, sample2, method='mean')

# Could also use method='median'
```

:::

::: {#Python .tabcontent}

```{r engine='python'}
# Function defined at 'https://raw.githubusercontent.com/ishaandey/nonparametric/main/helper.py'

sample1 = [37, 49, 55, 57]
sample2 = [23, 31, 39, 46]

permutation_test(sample1, sample2, method='mean')
```

:::
:::


## Variants {-}

Instead of difference in means, we could use either (1) sums, (2) trimmed means, or (3) medians:

- **Mean/Sum**: Use when pop. dist. is short-tailed (normal looking)
- **Trimmed Mean**: Use when pop. dist. is symmetric but heavy-tailed (some unusually extreme observations are likely)
- **Median**: Use when population distribution is skewed


# Wilcoxon Rank-Sum

Wilcoxon Rank-Sum is great for testing with low sample sizes and outliers, since it uses the rank of the observation as opposed to the value itself.

Only one **assumption**: both population distributions should be continuous (not categorical or discrete)

## How It Works

The goal here is to use *ranks*, not actual values, to identify differences in location. Why? Ranks are far more resistant to outliers, since a singly high observation is now just ranked at the max, doesn't matter *how far above* in absolute value it is.

We can pool the observations and compare the ranks that were assigned to sample 1 against those assigned to sample 2. We can compare the average rank of sample 1 against sample 2, and if it's lower by a particular margin, we can conclude that the values of sample 1 are below sample 2.

We'll use the following as an example:

```{r, echo=F, eval=T, results=T}
sample2 <- c(46, 48, 49, 51)
sample1 <- c(31, 32, 33, 47)

df <- tibble('Sample 1'=sample1, 'Sample 2'=sample2) %>% t()
kbl(df) %>% 
  column_spec(1, bold = T, border_right = T) %>%
  row_spec(1, color=col1) %>%
  row_spec(2, color=col3) %>%
  kable_styling(full_width = F, html_font = '"Karla", calibri, sans-serif')
```

We'll first calculate $W_{obs}$, our test statistic. To do so, we'll first pool both samples together and rank them, assigning a value of `1` to the smallest observation, and `m+n` to the largest (since there are now $m+n$ observations in the pooled group).

```{r, echo=F, eval=T, results=T}
pooled <- c(sample1, sample2)
ranks <- rank(pooled,ties.method = 'average')
df2 <- tibble('Values'=pooled, 'Ranks'=ranks, 'Sample'=c(rep('Sample1', 4), rep('Sample2', 4))) 
row.names(df2) <- NULL
df2$Values <- cell_spec(df2$Values, color = ifelse(df2$Sample == 'Sample1', col1, col3))
df2$Ranks <- cell_spec(df2$Ranks, color = ifelse(df2$Sample == 'Sample1', col1, col3), bold = ifelse(df2$Sample == 'Sample1', T, F))

kbl(df2[c('Values','Ranks')] %>% t(), escape=F, align='c') %>%
  kable_styling(full_width = F, html_font = '"Karla", calibri, sans-serif')
```

$W_{obs}$ is simply the *sum of ranks* of sample 1 observations: $W_{obs}=$ <span style="color: #14366e;">1+2+3+5</span> $=11$.

Why use this instead of comparing mean ranks? Turns out, the sum of ranks of one sample is a linear function of the mean, so there's a 1:1 correspondence between the two. The sum of ranks just simplifies the computation.

`r show_hr()`

Under the null hypothesis, we'd expect our observations to have no difference in ranks. So, if we were to randomly switch around (permute) the observations across the samples, we'd expect our observed test statistic $W_{obs}$ to not be anything unusual, according to $H_0$. In otherwords, random chance could have just as easily produced $W_{obs}$ as the treatment we gave.

Let's make that idea a little more quantitative: If sample 1 has $m$ observations, and sample 2 has $n$ observations, there are $\binom {m+n}{m} = \frac {(m+n)!}{m!n!}$ permutations when we pool together our observations and reassign them to a group. We can calculate a test statistic $W^*$ for each one of these permutations.

Our p-value is then just the *fraction of permutations* that have a test statistic $W$ as or more extreme than what was observed $W_{obs}$:

In our example, there are $\binom {8}{4}=70$ possible ways we could've obtained the four observations in sample 1 from a total of eight values. We could look up the significance from a table, or read off the p-value from the test output.


<details><summary> Formal Definitions </summary>
<p>

For a double sided test:
$$
H_0: F_1(x) = F_2(x) \\
H_a: F_1(x) \neq  F_2(x) \\
~ \\
p\text{-value}_{two\ sided} = \frac{\text{# of W's more extreme than } W_{obs} \text{ across both tails}}{\binom {m+n}{m}}
$$
For an upper tail test:
$$
H_0: F_1(x) = F_2(x) \\
H_a: F_1(x) \leq  F_2(x) \\
~ \\
p\text{-value}_{upper} = \frac{\text{# of }W\leq W_{obs}}{\binom {m+n}{m}}
$$

For a lower tail test:
$$
H_0: F_1(x) = F_2(x) \\
H_a: F_1(x) \geq  F_2(x) \\
~ \\
p\text{-value}_{lower} = \frac{\text{# of }W\geq W_{obs}}{\binom {m+n}{m}}
$$

</p>
</details>


**Interpretation**: Given a p-value of 0.028, there is a 2.8% chance of observing a difference as extreme as we did under the hypothesis that these samples come from populations with the same distribution. 

Because our p-value is less than our confidence threshold $\alpha$ of $0.05$, we reject the null hypothesis that $F_1(x) = F_2(x)$, and conclude that the location of population 1 is lower than population 2.

## Code 

::: {.tab}
<button class="tablinks" onclick="unrolltab(event, 'R')">R</button>
<button class="tablinks" onclick="unrolltab(event, 'Python')">Python</button>

::: {#R .tabcontent}

```{r, eval=T, include=T}
library(stats)

sample1 <- c(31, 32, 33, 47)
sample2 <- c(46, 48, 49, 51)

wilcox.test(sample1, sample2, alternative="less")
```

:::

::: {#Python .tabcontent}

```{r engine='python'}
from scipy.stats import ranksums

sample1 = [31, 32, 33, 47]
sample2 = [46, 48, 49, 51]

ranksums(sample1, sample2, alternative='less')
```

:::
:::


# Mann-Whitney 

The Mann-Whitney test is quite similar to the [Wilcoxon Rank-Sum test](wilcoxon-rank-sum.html).

We make the following **assumptions**:

1. Observations from groups are independent 
2. Both population distributions are continuous (not categorical / discrete)

## How It Works

In a sample of $m$ observations in sample $X$, and $n$ observations in sample $Y$, we want to focus on *each possible pair* of observations. The test statistic $U$ is simply the number of pairs where $X_i < Y_j$. The minimum $U$ can be is $0$, while the max is every possible pair, or $m*n$.

Let's say we have two samples, and want to see if sample 1 has a lower location than sample 2. Here's our raw data:

```{r, echo=F, eval=T, results=T}
library(dplyr)
sample2 <- c(39, 49, 55, 57)
sample1 <- c(31, 33, 46, 40)

df <- tibble('Sample 1' = sample1, 'Sample 2' = sample2) %>% t()
kbl(df) %>% 
  column_spec(1, bold=T, border_right = T) %>%
  row_spec(1, color=col1) %>%
  row_spec(2, color=col2) %>%
  kable_paper(full_width = F, html_font = '"Karla", calibri, sans-serif')
```

We look at every possible combination of the two samples and compare the values, checking if the values of the first sample are greater than the values of the second sample:

```{r, echo=F, eval=T, results=T}
comp.mat <- matrix(nrow=length(sample2), ncol=length(sample1))
rownames(comp.mat) <- sort(sample2) %>% as.character()
colnames(comp.mat) <- sort(sample1) %>% as.character()

for (i in sample1){
  for (j in sample2) {
    if (i > j) {
      comp <- 'Y'
    } 
    else {comp <- 'N'}
    comp.mat[as.character(j),as.character(i)] <- comp
  }
}

df = data.frame(comp.mat)
colnames(df) <- colnames(comp.mat)
rownames(df) <- rownames(comp.mat)


df2 = apply(df, 2, 
            function(x) {
              cell_spec(x, color = "white", align = "c", 
                        background = factor(x, c('N','Y'), c("#BBBBBB", colX)))
              }
            )
rownames(df2) <- rownames(comp.mat)

kbl(df2, format='html', escape=F) %>%
  kable_styling(full_width = F, html_font = '"Karla", calibri, sans-serif') %>%
  row_spec(0, bold=T, color=col1) %>%
  column_spec(1, bold=T, color = col2) %>%
  add_header_above(c("Is Sample 1 > Sample 2?" = 5))
```

Our test statistic $U_{obs}$ is the number of pairs where $X_i < Y_j$, indicated by the **Y**'s in the matrix. So,  $U_{obs}=2$.

`r show_hr()`

If we were to randomly permute the observations across both labels, we'd need to see that our observed test statistic is far more extreme that the rest of the U statistics before concluding that the null hypothesis doesn't apply. Why? The null hypothesis suggests that the observations from both samples are derived from the same distribution, so we need sufficient evidence that this isn't the case in order to reject it.

```{r,echo=F, eval=T, results=T}
perm.num <- c(1:3, fa('ellipsis-v', fill='black'), 70)
U.stat <- c(0:1,1, fa('ellipsis-v', fill='black'), 16)
df <- data.frame('Permutation'=perm.num, 'U'=U.stat) 
kbl(df, format='html', escape=F, align='c') %>% 
  kable_styling(full_width = F, html_font = '"Karla", calibri, sans-serif',
                position='float_right')
```

Our observed sample assignment is just one of $\binom{8}{4}=70$ possible permutations of the values between the two sample labels. Let's go ahead and find the corresponding test statistic $U^*$ for each of the other permutations:

So, the probability that we observe $U_{obs}=2$ is then the number of $U$'s less than $U_{obs}$. Since there are 4 of 70 possible permutations with $U^* \leq U_{obs}$, we get a p-value of $\frac{4}{70}=0.057$.

The intuition here is fairly straightforward: We expect to see a test statistic $U$ as or more extreme than $U_{obs}$ 5.7% of the time when we assume that there is no difference in the null hypothesis.

<details><summary> Formal Definitions </summary>
<p>

For a double sided test:
$$
H_0: F_1(x) = F_2(x) \\
H_a: F_1(x) \neq  F_2(x) \\
~ \\
p\text{-value}_{two\ sided} = \frac{\text{# of U's farther from  } \frac{mn}{2}}{\binom {m+n}{m}} \\
$$
For an upper tail test:
$$
H_0: F_1(x) = F_2(x) \\
H_a: F_1(x) \leq  F_2(x) \\
~ \\
p\text{-value}_{upper} = \frac{\text{# of }U\leq U_{obs}}{\binom {m+n}{m}}
$$

For a lower tail test:
$$
H_0: F_1(x) = F_2(x) \\
H_a: F_1(x) \geq  F_2(x) \\
~ \\
p\text{-value}_{lower} = \frac{\text{# of }U\geq U_{obs}}{\binom {m+n}{m}}
$$

</p>
</details>


## Code 

```{r}
sample1 <- c(31, 33, 46, 40)
sample2 <- c(39, 49, 55, 57)

wilcox.test(sample1, sample2, alternative='less')
```


## Note

The Wilcoxon $W$ is linearly related to Mann Whitney $U$, and **results in the same p-value**.

<details><summary> Proof </summary>
<p>
$$
\begin{array}{l}
W_{2} \\
=\sum_{j=1}^{n} R\left(Y_{j}\right) \\
=R\left(Y_{1}\right)+R\left(Y_{2}\right)+\cdots+R\left(Y_{n}\right) \\
=\left[1+\left(\text {number of } X^{\prime} s \leq Y_{1}\right)\right]+\left[2+\left(\text {number of } X^{\prime} s \leq Y_{2}\right)\right]+\cdots \\
=[1+\cdots+n]+\left[\left(\text {number of } X^{\prime} s \leq Y_{1}\right)+\cdots+\left(\text {number of } X^{\prime} s \leq Y_{n}\right)\right] \\
=[1+\cdots+n]+U \\
=\frac{n(n+1)}{2}+U
\end{array}
$$

</p>
</details>
