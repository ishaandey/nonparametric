[["index.html", "Nonparametric Handbook A Data Analyst’s Guide to Statistical Testing Introduction Why This Book? Who Are You? Who Are We?", " Nonparametric Handbook A Data Analyst’s Guide to Statistical Testing Ishaan Dey January 2021 Introduction Why This Book? What Does It Cover Who Are You? Who Are We? "],["cheat-sheet.html", "Cheat Sheet", " Cheat Sheet add flowchart "],["statistical-foundations.html", "Statistical Foundations Testing Basics Math Notation", " Statistical Foundations Testing Basics This handbook assumes knowledge of basic statistical testing, ideas such as p-values and hypothesis testing. Here’s a quick refresher: Samples and Populations Say you and your buddy get in a disagreement: You claim to shoot over 50% from the free-throw line, your friend thinks otherwise. Since we don’t actually know what your true ability is, or the proportion of shots you can make, we have to estimate it somehow. Let’s denote the true ability as \\(p\\). This is a population parameter, the value we’re trying to estimate. The both of you hit the courts, and shoot 30 balls. We can take the proportion of shots you make as \\(\\hat{p}\\), called a test statistic. These 30 shots are a sample of all the shots you’ve ever taken. The fundamental idea behind statistical testing is that we can estimate \\(p\\) from \\(\\hat{p}\\). Hypotheses The first step is to develop a set of hypotheses. The Null Hypothesis, \\(H_0\\), indicates our default belief. These are typically structured where the population parameter equals some value, i.e. \\(p=0.5\\). We could write this succinctly as \\(H_0: p=0.5\\). Why is it incorrect to say \\(H_0: \\hat{p}=0.5\\)? As you recall, \\(\\hat{p}\\) is the test statistic we’ve generated from our sample, so it’s fairly obvious if it differs from \\(0.5\\). Only if there is sufficiently evidence to overturn the null, we can reject it in favor of the Alternative Hypothesis, \\(H_a\\). This can have a range of values that reflects our expectations. If the argument is that your ability is greater than 50%, you’ll use a “greater than” hypothesis, i.e. \\(H_a: p &gt; 0.5\\). Similarly, we can have a “less than” direction with \\(H_a: p &lt; 0.5\\). If we don’t have a specified direction, simply that your ability is not 50%, we can use a “two sided” hypothesis, \\(H_a: p \\neq 0.5\\). This also works if we’re just looking for significance in either direction, without presupposing which. For the time being, let’s use the following hypotheses: \\[ H_0: p = 0.5 \\\\ H_a: p &gt; 0.5 \\] Confidence Level Say you shoot 16 of 30 shots, so \\(\\hat{p}=0.53\\). There’s some evidence that you shoot better than 50%, but its not as convincing as if you were to make 21 of 30 shots \\(\\hat{p}=0.67\\). We could conclude your true ability is above 50% in either case, but it’s far more likely you’re making an incorrect conclusion in the first scenario. Let’s set our tolerance for errors at a certain level, \\(\\alpha\\). \\(\\alpha\\) is known as the false positive rate, or probability of making a Type I error. All that says is when \\(H_0\\) is true (i.e. \\(p\\) really does \\(=0.5\\)), we expect to incorrectly reject \\(H_a\\) about \\(\\alpha\\)% of the time. By convention, we set \\(\\alpha=0.05\\), but other values may be appropriate given the context. As an aside, \\(\\alpha=0.05\\) implies a confidence level of \\(100-\\alpha = 95\\%\\). Testing A recap of our process so far: We are testing the hypothesis that \\(p &gt; 0.5\\), against the null \\(p=0\\). We took a sample of \\(n=\\) 30 shots, and made 21 of them. Our test statistic is \\(\\hat{p}=\\)\\(\\frac{21}{30}=\\)\\(0.70\\). We specified a confidence level of 95%, so we get \\(\\alpha=0.05\\). In order to reject the null in favor of the alternative, we’re looking for sufficient evidence from our test statistic. Consider this: if the test statistic did indeed come from a distribution where the true parameter is \\(p=0.5\\), how likely is it that we’d get \\(\\hat{p}\\) of \\(0.70\\) or greater? By the distribution specified by the null hypothesis (\\(\\hat{p} \\sim Binomial(n=30, p=0.5)\\)), the probability of getting a test statistic as or more extreme than 0.70 is 0.014. That’s a pretty low number, meaning it’s very unusual to be making 21 of 30 shots when we’re expecting to hit 15. This evidence against the null is called the p-value, and was generated from the test statistic. Because our p-value was less than alpha, we see that there is sufficient evidence to reject the null hypothesis, that \\(p=0.5\\). We thus conclude that \\(p\\) must be \\(&gt;0.50\\). That’s it! If you were able to follow along thus far, you’ve got the necessary background for the rest of the handbook. If not, give this section another look over, or check out some great online resources at Khan Academy. Math Notation In this section, you’ll may across some notation rooted in probability. Don’t be intimidated. Everyone has their own comfort level with math. Instead of omitting it entirely, we’ve kept in the formulas as details for the curious. Here’s some basic pointers, but if there’s something you don’t understand, you can probably skip it without concern. Indices. Counters like \\(i\\) or \\(j\\) just refer to a particular observation in a set. For example, \\((x_i,y_j)\\) can be read as “some pair from two sets”, where \\(x_i\\) is a part of \\(X\\), and \\(y_j\\) is a part of \\(Y\\). Sums. Something like \\(\\sum_{i=1}^n x_i\\) can be read as “add together each observation \\(x_i\\) in the set \\(X\\)”. Ranks: \\(R(X_i)\\) can be read as the rank of observation \\(i\\) in sample \\(X\\). If we have \\(n\\) observations, the ranks go in ascending order, \\(1 \\dots n\\). Combinations. Something like \\(\\binom{5}{2}\\) is read as “Five choose two”, or the probability of selecting a combination of 2 items out of 5 possibilities. More generally: \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\) Probability Distribution Function (PDF). \\(f_x(x)\\) describes the relative likelihood (not probability) of seeing the value \\(x\\), when \\(x\\) is continuous. It is always \\(\\geq0\\), and the integral across all values \\(\\int_{-\\infty}^{\\infty}f_x(x)dx\\) must \\(=1\\). Cumulative Distribution Function (CDF). \\(F_x(x)\\) describes the probability of seeing a value less than or equal to \\(x\\), \\(F_x(x) = Pr(X\\leq x)\\). Mathematically, it is the integral of the pdf \\(f_x(x)\\) for each point \\(x\\): \\(F_x(x) = \\int_{-\\infty}^{x}f_t(t)dt\\). To visualize that relationship: "],["introduction-1.html", "Introduction Background Notation Diagnostics", " Introduction Goal: We’ve got two samples, and want to see if there’s the significance difference in means. Background Comparing differences in means is one of the most commonly used procedures in statistics. Take the following use case: Does Product A have better ratings than than Product B? Sure, we could compare averages (i.e. Product A’s mean is \\(4.83\\), better than Product B’s \\(4.79\\)), but it fails to answer the question: Are the population ratings for A better than B? We only have data from a sample of reviews, so we’ve got to somehow estimate the population differences in ratings. Any student of introductory statistics knows the remedy: take into account the spread of the data using a two-sample t-test. But the t-test makes a few strong assumptions about the data, mainly the assumption of normality. This chapter describes alternative tests we can use instead. Notation Say you’re expecting population 1 to be lower than population 2. You’ll use a lower-tail test, and use a set of hypotheses like so: \\[ \\text{Null Hypothesis }H_0: \\mu_1 = \\mu_2 \\\\ Alternate Hypothesis H_a: \\mu_1 \\leq \\mu_2 \\] In this section, we don’t necessarily know the distribution of our data, but we’re testing that the distributions, not necessarily means, are equal. As such, our hypotheses will look something like: \\[ H_0: F_1(x) = F_2(x) \\\\ H_a: F_1(x) \\geq F_2(x) \\\\ \\text{ with a strict inequality for at least one }x \\] \\(F(x)\\) here refers to the empirical CDF, or the probability of seeing a value less than or below \\(x\\). If the location of population 1 is below population 2, we’d expect that at a certain point \\(x\\), the probability of seeing a value at or below \\(x\\) should be higher for population 1 than it is for population 2. Here’s a visual: From the first plot we see \\(\\mu_1 &lt; \\mu_2\\), and from the second, \\(F_1(x) \\geq F_2(x)\\). Both correspond to the “less than” hypothesis. Diagnostics A big assumption parametric tests take is that the data is normally distributed. There’s a few approaches we can take to validate this. First is the QQ plot: We could also use a more rigorous test of normality: But its almost always just worth checking the distribution regardless: "],["choosing-a-test.html", "Choosing a Test Power", " Choosing a Test Are your population distributions normal, or, are there more than 40 observations in each sample? If so, choose the Two Sample t-Test. If not, read on: Do you care about the magnitude of the difference? Consider the Permutation Test. Check the distribution and number of outliers. If the data is skewed, use the median variant, and if there are outliers on both tails, consider using trimmed means. If you’re instead just looking to determine if one sample is greater than the other, use the Wilcoxon Rank-Sum, particularly with skewed distributions or heavy outliers. If we’re interested in generating a confidence interval for the difference, we can use the Mann-Whitney test. It works similarly to the Wilcoxon Rank-Sum, and results in the same p-value. Power Include a discussion about power here "],["two-sample-t-test.html", "Chapter 1 Two Sample t-Test How It Works Code", " Chapter 1 Two Sample t-Test The two-sample t-test is by far the most powerful test when the assumptions are met for comparing means between two samples. Assumptions: Random sample from each population Both samples are independent Both population distributions are normal Both population variances are equal Note: By the Central Limit Theorem, we can assume that the sample means will start looking normal at large sample sizes (\\(n \\geq 40\\)). How It Works Do we know the population variance \\(\\sigma^2\\)? - If so, we’ll use the \\(z\\) distribution: \\(z\\sim N(0,1)\\) - Otherwise, we’ll use the \\(t\\) distribution, \\(t\\sim t(df)\\), where \\(df\\) is the minimum of the two sample sizes - 1. Code R Python library(stats) sample1 &lt;- c(1.1, 2.1, 4.2, 3.2, 1.7, 2.2, 2.7) sample2 &lt;- c(3.9, 2.9, 3.8, 1.8, 3.3, 2.8, 2.3) t.test(x=sample1, y=sample2, alternative=&#39;two.sided&#39;, paired=F) from scipi.stats import ttest_ind sample1 = [1.1, 2.1, 4.2, 3.2, 1.7, 2.2, 2.7] sample2 = [3.9, 2.9, 3.8, 1.8, 3.3, 2.8, 2.3] ttest_ind(a=sample1, b=sample2) "],["two-sample-permutation.html", "Chapter 2 Permutation Test How It Works Code Variants", " Chapter 2 Permutation Test Use the permutation test if the normality assumption is violated, and you’re interested in quantifying the difference in some location parameter: mean, trimmed mean, or median. This works well for smaller sample sizes. Assumptions: Random sample from each population Both are sampled independently Both population distributions are continuous (not categorical / discrete) Note: We no longer need the assumption of normality, nor equal variances How It Works We’ll use \\(D_{obs}\\) to represent the difference in means that we observe between our samples. For the following samples, we find \\(D_{obs}=12.5\\). Sample A 31 32 34 47 Mean: 36 Sample B 46 48 49 51 Mean: 48.5 Under the null hypothesis, we’d expect that there is no difference in means. In other words, we could randomly switch around the values across the samples and generate test statistics \\(D^*\\) for each permutation. If the null hypothesis is false, and the difference we observe can’t be explained by random chance, we’d see that only a few \\(D^*\\)’s are more extreme than \\(D_{obs}\\). The way we do this is fairly straightforward. We’ll first pool together our observed values. Pooled 31 32 34 47 46 48 49 51 Now we’ll create as many permutations as we can, reassigning the “Sample A” and “Sample B” labels across all the observations. There are \\(\\binom {m+n}{m} = \\binom {8}{4} = 70\\) possible permutations. We’ll calculate a distribution of test statistics by finding what \\(D^*\\) is for each permutation: Permutation 1 Permutation 2 … Permutation 70 A* B* A** B** etc A*** B*** 46 31 46 31 … 47 31 32 48 48 32 … 48 32 34 49 34 49 … 49 34 47 51 47 51 … 51 46 Permutation 1 Permutation 2 … Permutation 70 D*= -5 D*= 3 … D*= 13 From our calculated \\(D^*\\)’s, we find that there are only 4 permutations that yield a test statistic greater than \\(D_{obs}\\). So, our p-value is simply \\(\\frac{4}{70}= 0.057\\). Formal Definitions For a double sided test: \\[ H_0: F_1(x) = F_2(x) \\\\ H_a: F_1(x) \\neq F_2(x) \\\\ ~ \\\\ p\\text{-value}_{two\\ sided} = \\frac{\\text{# of |D&#39;s|}~\\geq~|D_{obs}|}{\\binom {m+n}{m}} \\] For an upper tail test: \\[ H_0: F_1(x) = F_2(x) \\\\ H_a: F_1(x) \\leq F_2(x) \\\\ ~ \\\\ p\\text{-value}_{upper} = \\frac{\\text{# of }D\\geq D_{obs}}{\\binom {m+n}{m}} \\] For a lower tail test: \\[ H_0: F_1(x) = F_2(x) \\\\ H_a: F_1(x) \\geq F_2(x) \\\\ ~ \\\\ p\\text{-value}_{lower} = \\frac{\\text{# of }D\\leq D_{obs}}{\\binom {m+n}{m}} \\] Interpretation: Given a p-value of 0.057, there is a 5.7% chance of observing a difference as extreme as we did under the hypothesis that these samples come from populations with the same distribution. Code R Python source(&#39;https://raw.githubusercontent.com/ishaandey/nonparametric/master/helper.R&#39;) sample1 &lt;- c(46, 48, 49, 51) sample2 &lt;- c(31, 32, 34, 47) permutation_test(sample1, sample2, method=&#39;mean&#39;) # Could also use method=&#39;median&#39; # Function defined at &#39;https://raw.githubusercontent.com/ishaandey/nonparametric/main/helper.py&#39; sample1 = [37, 49, 55, 57] sample2 = [23, 31, 39, 46] permutation_test(sample1, sample2, method=&#39;mean&#39;) Variants Instead of difference in means, we could use either (1) sums, (2) trimmed means, or (3) medians: Mean/Sum: Use when pop. dist. is short-tailed (normal looking) Trimmed Mean: Use when pop. dist. is symmetric but heavy-tailed (some unusually extreme observations are likely) Median: Use when population distribution is skewed "],["wilcoxon-rank-sum.html", "Chapter 3 Wilcoxon Rank-Sum 3.1 How It Works 3.2 Code", " Chapter 3 Wilcoxon Rank-Sum Wilcoxon Rank-Sum is great for testing with low sample sizes and outliers, since it uses the rank of the observation as opposed to the value itself. Only one assumption: both population distributions should be continuous (not categorical or discrete) 3.1 How It Works The goal here is to use ranks, not actual values, to identify differences in location. Why? Ranks are far more resistant to outliers, since a singly high observation is now just ranked at the max, doesn’t matter how far above in absolute value it is. We can pool the observations and compare the ranks that were assigned to sample 1 against those assigned to sample 2. We can compare the average rank of sample 1 against sample 2, and if it’s lower by a particular margin, we can conclude that the values of sample 1 are below sample 2. We’ll use the following as an example: Sample 1 31 32 33 47 Sample 2 46 48 49 51 We’ll first calculate \\(W_{obs}\\), our test statistic. To do so, we’ll first pool both samples together and rank them, assigning a value of 1 to the smallest observation, and m+n to the largest (since there are now \\(m+n\\) observations in the pooled group). Values 31 32 33 47 46 48 49 51 Ranks 1 2 3 5 4 6 7 8 \\(W_{obs}\\) is simply the sum of ranks of sample 1 observations: \\(W_{obs}=\\) 1+2+3+5 \\(=11\\). Why use this instead of comparing mean ranks? Turns out, the sum of ranks of one sample is a linear function of the mean, so there’s a 1:1 correspondence between the two. The sum of ranks just simplifies the computation. Under the null hypothesis, we’d expect our observations to have no difference in ranks. So, if we were to randomly switch around (permute) the observations across the samples, we’d expect our observed test statistic \\(W_{obs}\\) to not be anything unusual, according to \\(H_0\\). In otherwords, random chance could have just as easily produced \\(W_{obs}\\) as the treatment we gave. Let’s make that idea a little more quantitative: If sample 1 has \\(m\\) observations, and sample 2 has \\(n\\) observations, there are \\(\\binom {m+n}{m} = \\frac {(m+n)!}{m!n!}\\) permutations when we pool together our observations and reassign them to a group. We can calculate a test statistic \\(W^*\\) for each one of these permutations. Our p-value is then just the fraction of permutations that have a test statistic \\(W\\) as or more extreme than what was observed \\(W_{obs}\\): In our example, there are \\(\\binom {8}{4}=70\\) possible ways we could’ve obtained the four observations in sample 1 from a total of eight values. We could look up the significance from a table, or read off the p-value from the test output. Formal Definitions For a double sided test: \\[ H_0: F_1(x) = F_2(x) \\\\ H_a: F_1(x) \\neq F_2(x) \\\\ ~ \\\\ p\\text{-value}_{two\\ sided} = \\frac{\\text{# of W&#39;s more extreme than } W_{obs} \\text{ across both tails}}{\\binom {m+n}{m}} \\] For an upper tail test: \\[ H_0: F_1(x) = F_2(x) \\\\ H_a: F_1(x) \\leq F_2(x) \\\\ ~ \\\\ p\\text{-value}_{upper} = \\frac{\\text{# of }W\\leq W_{obs}}{\\binom {m+n}{m}} \\] For a lower tail test: \\[ H_0: F_1(x) = F_2(x) \\\\ H_a: F_1(x) \\geq F_2(x) \\\\ ~ \\\\ p\\text{-value}_{lower} = \\frac{\\text{# of }W\\geq W_{obs}}{\\binom {m+n}{m}} \\] Interpretation: Given a p-value of 0.028, there is a 2.8% chance of observing a difference as extreme as we did under the hypothesis that these samples come from populations with the same distribution. Because our p-value is less than our confidence threshold \\(\\alpha\\) of \\(0.05\\), we reject the null hypothesis that \\(F_1(x) = F_2(x)\\), and conclude that the location of population 1 is lower than population 2. 3.2 Code R Python library(stats) sample1 &lt;- c(31, 32, 33, 47) sample2 &lt;- c(46, 48, 49, 51) wilcox.test(sample1, sample2, alternative=&quot;less&quot;) Wilcoxon rank sum exact test data: sample1 and sample2 W = 1, p-value = 0.02857 alternative hypothesis: true location shift is less than 0 from scipy.stats import ranksums sample1 = [31, 32, 33, 47] sample2 = [46, 48, 49, 51] ranksums(sample1, sample2, alternative=&#39;less&#39;) "],["mann-whitney.html", "Chapter 4 Mann-Whitney 4.1 How It Works 4.2 Code 4.3 Note", " Chapter 4 Mann-Whitney The Mann-Whitney test is quite similar to the Wilcoxon Rank-Sum test. We make the following assumptions: Observations from groups are independent Both population distributions are continuous (not categorical / discrete) 4.1 How It Works In a sample of \\(m\\) observations in sample \\(X\\), and \\(n\\) observations in sample \\(Y\\), we want to focus on each possible pair of observations. The test statistic \\(U\\) is simply the number of pairs where \\(X_i &lt; Y_j\\). The minimum \\(U\\) can be is \\(0\\), while the max is every possible pair, or \\(m*n\\). Let’s say we have two samples, and want to see if sample 1 has a lower location than sample 2. Here’s our raw data: Sample 1 31 33 46 40 Sample 2 39 49 55 57 We look at every possible combination of the two samples and compare the values, checking if the values of the first sample are greater than the values of the second sample: Is Sample 1 &gt; Sample 2? 31 33 40 46 39 N N Y Y 49 N N N N 55 N N N N 57 N N N N Our test statistic \\(U_{obs}\\) is the number of pairs where \\(X_i &lt; Y_j\\), indicated by the Y’s in the matrix. So, \\(U_{obs}=2\\). If we were to randomly permute the observations across both labels, we’d need to see that our observed test statistic is far more extreme that the rest of the U statistics before concluding that the null hypothesis doesn’t apply. Why? The null hypothesis suggests that the observations from both samples are derived from the same distribution, so we need sufficient evidence that this isn’t the case in order to reject it. Permutation U 1 0 2 1 3 1 70 16 Our observed sample assignment is just one of \\(\\binom{8}{4}=70\\) possible permutations of the values between the two sample labels. Let’s go ahead and find the corresponding test statistic \\(U^*\\) for each of the other permutations: So, the probability that we observe \\(U_{obs}=2\\) is then the number of \\(U\\)’s less than \\(U_{obs}\\). Since there are 4 of 70 possible permutations with \\(U^* \\leq U_{obs}\\), we get a p-value of \\(\\frac{4}{70}=0.057\\). The intuition here is fairly straightforward: We expect to see a test statistic \\(U\\) as or more extreme than \\(U_{obs}\\) 5.7% of the time when we assume that there is no difference in the null hypothesis. Formal Definitions For a double sided test: \\[ H_0: F_1(x) = F_2(x) \\\\ H_a: F_1(x) \\neq F_2(x) \\\\ ~ \\\\ p\\text{-value}_{two\\ sided} = \\frac{\\text{# of U&#39;s farther from } \\frac{mn}{2}}{\\binom {m+n}{m}} \\\\ \\] For an upper tail test: \\[ H_0: F_1(x) = F_2(x) \\\\ H_a: F_1(x) \\leq F_2(x) \\\\ ~ \\\\ p\\text{-value}_{upper} = \\frac{\\text{# of }U\\leq U_{obs}}{\\binom {m+n}{m}} \\] For a lower tail test: \\[ H_0: F_1(x) = F_2(x) \\\\ H_a: F_1(x) \\geq F_2(x) \\\\ ~ \\\\ p\\text{-value}_{lower} = \\frac{\\text{# of }U\\geq U_{obs}}{\\binom {m+n}{m}} \\] 4.2 Code sample1 &lt;- c(31, 33, 46, 40) sample2 &lt;- c(39, 49, 55, 57) wilcox.test(sample1, sample2, alternative=&#39;less&#39;) 4.3 Note The Wilcoxon \\(W\\) is linearly related to Mann Whitney \\(U\\), and results in the same p-value. Proof \\[ \\begin{array}{l} W_{2} \\\\ =\\sum_{j=1}^{n} R\\left(Y_{j}\\right) \\\\ =R\\left(Y_{1}\\right)+R\\left(Y_{2}\\right)+\\cdots+R\\left(Y_{n}\\right) \\\\ =\\left[1+\\left(\\text {number of } X^{\\prime} s \\leq Y_{1}\\right)\\right]+\\left[2+\\left(\\text {number of } X^{\\prime} s \\leq Y_{2}\\right)\\right]+\\cdots \\\\ =[1+\\cdots+n]+\\left[\\left(\\text {number of } X^{\\prime} s \\leq Y_{1}\\right)+\\cdots+\\left(\\text {number of } X^{\\prime} s \\leq Y_{n}\\right)\\right] \\\\ =[1+\\cdots+n]+U \\\\ =\\frac{n(n+1)}{2}+U \\end{array} \\] "],["summary.html", "Summary", " Summary Goal: some "],["parametric-paired-t-test.html", "Chapter 5 Parametric Paired T-Test 5.1 Usage 5.2 Procedure 5.3 Code", " Chapter 5 Parametric Paired T-Test 5.1 Usage 5.1.1 Assumptions Paired observations are a random sample (independent) from population of all possible pairs The differences are normally distributed Note: We can assume the sample mean will approach a normal distribution when \\(n_d \\geq 40\\) 5.2 Procedure Paired t-test statistic: \\[ t=\\frac{\\bar{x}_{d}}{S_{d} / \\sqrt{n_{d}}} \\sim t\\left(n_{d}-1\\right) \\] \\(\\bar{x}_{d}\\) is the sample mean difference. \\(s_{d}\\) is the sample standard deviation of the differences. \\(n_{d}\\) is the number of pairs. We’re trying to test the hypothesis: \\[ \\begin{aligned} H_0&amp;: \\mu_d = 0 \\\\ H_a&amp;: \\mu_d &gt; 0,~&lt;0,~\\text{or}~\\neq 0 \\end{aligned} \\] 5.3 Code 5.3 R library(stats) samp.before &lt;- c(1.1, 2.1, 4.2, 3.2, 1.7, 2.2, 2.7) samp.after &lt;- c(3.9, 2.9, 3.8, 1.8, 3.3, 2.8, 2.3) t.test(x=samp.before, y=samp.after, alternative=&#39;two.sided&#39;, paired=T) 5.3 Python from scipi.stats import ttest_rel samp_before = [1.1, 2.1, 4.2, 3.2, 1.7, 2.2, 2.7] samp_after = [3.9, 2.9, 3.8, 1.8, 3.3, 2.8, 2.3] ttest_rel(samp_before, samp_after) "],["paired-permutation-test.html", "Chapter 6 Paired Permutation Test 6.1 Usage 6.2 Procedure 6.3 Code", " Chapter 6 Paired Permutation Test 6.1 Usage By convention, differences are defined as \\(\\text{treatment} - \\text{control}\\). We can use this test if we violate the normality assumption of the paired t-test. However, as we’re still using an average of differences, our test statistic is still subject to outliers. 6.2 Procedure We’ll define \\(D_{obs}\\) as the average of differences between our samples \\[ \\bar{D}_{obs}=\\frac{\\sum_{i=1}^{n} D_{i}}{n} \\] Under the null hypothesis, we’d expect that if we were to randomly switch around (permute) the observed within the pair, we’d see the same test statistic. Since we have \\(n\\) pairs, there are \\(2^n\\) possible arrangements where we swap around the values across the treatment and control group. For each permutation, we’ll find \\(\\bar{D}^*\\), the mean of differences for that particular permutation. More visually: Obs 1 Obs 2 Obs 3 Obs 4 Obs 5 before 31 38 46 54 43 after 39 49 55 57 44 can be permuted within pairs, with one permutation looking like: Obs * Obs 2 Obs * Obs 4 Obs 5 before* 39 38 55 54 43 after* 31 49 46 57 44 but not across pairs, because we’re using a matched pairs design. So, our p-value is then just the fraction of permutations that have a test statistic \\(D\\) as or more extreme than what was observed \\(D_{obs}\\). P-value Formula \\[ \\begin{aligned} P_{\\text {upper tail}} &amp;= \\frac{\\text {number of } \\bar{D}^{\\prime} s \\geq \\bar{D}_{obs}}{2^{n}} \\\\ P_{\\text {lower tail}} &amp;= \\frac{\\text {number of } \\bar{D}^{\\prime} s \\leq \\bar{D}_{obs}}{2^{n}} \\end{aligned} \\] 6.3 Code 6.3 R library(stats) samp.before &lt;- c(1.1, 2.1, 4.2, 3.2, 1.7, 2.2, 2.7) samp.after &lt;- c(3.9, 2.9, 3.8, 1.8, 3.3, 2.8, 2.3) t.test(x=samp.before, y=samp.after, alternative=&#39;two.sided&#39;, paired=T) 6.3 Python # under construction "],["wilcoxon-signed-rank-test.html", "Chapter 7 Wilcoxon Signed-Rank Test 7.1 Usage 7.2 Procedure", " Chapter 7 Wilcoxon Signed-Rank Test 7.1 Usage Signed Ranks are a method of ranking matched pairs data while accounting for the positive or negative nature of the differences. In other words, we can take into account if the treatment or control was higher, without caring about how much higher it was. 7.2 Procedure For each pair of observations, we’ll take the difference as after - before. We’ll then rank these differences by absolute value, smallest to largest, but retain the sign of the rank from the actual difference. Our test statistic, \\(SR_+\\), is the sum of positive signed ranks. More visually: Obs 1 Obs 2 Obs 3 Obs 4 Obs 5 before 31 38 46 54 45 after 39 49 55 57 43 becomes Obs 1 Obs 2 Obs 1 Obs 4 Obs 5 before 31 38 46 54 45 after 39 49 55 57 43 diff 8 11 9 3 -2 sign.rank 3 5 4 2 -1 Here the the test statistic is found as \\(SR_+ = 3+5+4+2 \\implies SR_+=14\\). In order to find the p-value, we want to ask how likely is it that we found this test statistic just by random chance, assuming the null to be true? In other words, we’ll find every possible permutation of the sign-ranks, and calculate \\(SR_+^*\\) for each one. The p-value is just the fraction of ranks sums as or more extreme than what we observed. P-value Formula \\[ \\begin{aligned} P_{\\text {upper tail}} &amp;=\\frac{\\text {number of } SR_+ \\text{&#39;s} \\geq SR_+}{2^{n}} \\\\ P_{\\text {lower tail}} &amp;=\\frac{\\text {number of } SR_+ \\text{&#39;s} \\leq SR_+}{2^{n}} \\end{aligned} \\] A brief comment on ties: Details Note: When we have multiple pairs with the same difference, we can apply the average rank to the tied observations. Note: If the difference between a pair is 0, we either omit them from the sample, or ignore them (as they don’t count towards \\(SR_+\\)). You’d typically keep them since it gives similar results, and if there many zeros, you’d lose plenty of power from the lower sample size. 7.2 R library(stats) pre &lt;- c(1180, 1210, 1300, 1080, 1120, 1240, 1360, 980) post &lt;- c(1230, 1280, 1310, 1140, 1150, 1200, 1340, 1100) diff &lt;- post-pre wilcox.test(diff, alternative=&quot;greater&quot;) 7.2 Python # under construction "],["sign-test.html", "Chapter 8 Sign Test 8.1 Usage 8.2 Procedure 8.3 Code", " Chapter 8 Sign Test 8.1 Usage 8.2 Procedure 8.2.1 Hypothesis \\(H_{o}: F(x)=1-F(-x)\\) \\(H_{a}: F(x) \\leq 1-F(-x)\\) or \\(H_{a}: F(x) \\geq 1-F(-x)\\) 8.2.2 Test statistic \\(\\mathrm{SN}_{+}\\) is the number of observations greater than 0 If \\(\\mathrm{H}_{\\mathrm{o}}\\) is true, then the distribution of \\(\\mathrm{SN}_{+}\\) is: \\(SN_+ \\sim \\text{Binom}(n, 0.5)\\) or \\(SN_+ \\sim N(0.5 n, \\sqrt{0.25 n})\\) for large enough samples 8.2.3 p value \\[ P_{\\text {upper tail}}=P(SN_+ \\geq SN_{+,obs})\\\\ P_{\\text {lower tail}}=P(SN_+ \\leq SN_{+,obs}) \\] 8.3 Code 8.3 R library(stats) before &lt;- c(1180, 1210, 1300, 1080, 1120, 1240, 1360, 980) after &lt;- c(1230, 1280, 1310, 1140, 1150, 1200, 1340, 1100) diff &lt;- after - before n &lt;- length(diff) SN &lt;- length(diff[diff &gt; 0]) 1 - pbinom(SN, n, 0.5) 8.3 Python # under construction "],["choosing-which-test.html", "Chapter 9 Choosing Which Test", " Chapter 9 Choosing Which Test Look at distribution of differences: If light-tailed and symmetric \\(\\implies\\) Use paired permutation test If skewed or heavy tailed \\(\\implies\\) Use wilcoxon signed-rank test Sign test also is efficient, but generally has low power for smaller \\(n\\) If normal looking \\(\\implies\\) Use sign test "],["summary-sheet.html", "Summary Sheet Background When To Use What", " Summary Sheet How do we test for non-linear association? Background Sometimes the data is strongly correlated, but there is a non-linear pattern. Take the following example: 1 2 3 4 5 6 1 16 81 256 625 1296 There is a perfect relationship between the data: \\(y=x^4\\). Yet the correlation, calculated from a linear fit, doesn’t quite reflect that: \\(\\rho=\\) 0.896. What we’re in need of is an improvement of Pearson’s correlation. We’d still like it to describe what happens to \\(Y\\) when \\(X\\) increases, but relax the assumption that the relationship needs to be linear. When To Use What "],["parametric-ols.html", "Chapter 10 Parametric OLS 10.1 Usage 10.2 How It Works 10.3 Code 10.4 Note", " Chapter 10 Parametric OLS 10.1 Usage 10.1.1 Assumption 10.2 How It Works When we fit a line to a slope, we can extract coefficients from our data. Let’s take oft-used mtcars dataset as an example, regressing log(mpg) against log(disp). We can fit a line through this, shown in blue. data(mtcars) model = lm(log(mtcars$mpg)~log(mtcars$disp)) mtcars %&gt;% ggplot(aes(x = log(disp), y = log(mpg))) + geom_point() + geom_smooth(method = &quot;lm&quot;, fill = NA) The equation of this line tells us some important information. Given in the form \\(Y_i = \\beta_0 + \\beta_1X_i+\\varepsilon_i\\), we can estimate coefficients from our linear model as \\(\\hat{\\beta_0}=\\) 5.381 and \\(\\hat{\\beta_1}=\\) -0.459. Crucially: If \\(\\beta_1=0\\), \\(Y\\) doesn’t depend on \\(X_1\\). So naturally, we ask the question: “How well does my estimate of the slope, or \\(\\hat{\\beta_1}\\), actually represent \\(\\beta_1\\)”? In other words, we want to test if \\(\\hat{\\beta_1}\\) is significantly different from \\(0\\). How? We’ll use the t statistic (formula below for those interested) \\[ t_{\\text {slope}} = \\hat{\\beta}_{1} / \\sqrt{ \\frac{\\frac{1}{n-2}{\\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y_{i}}\\right)^{2}}} {{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}}} } \\] Assuming our errors, \\(\\varepsilon_i\\), are normally distributed about \\(0\\), our \\(t\\) statistic should follow a t-distribution: \\(t_{\\text {slope}}\\sim t(n-2)\\) 10.3 Code Most regression functions make this super easy to implement. 10.4 Note Pearson’s Correlation is a refinement of our basic OLS model, particularly when there is just 1 explanatory variable \\(X_1\\). When we do so, the \\(R^2\\) of our model is actually just square of the correlation of our variables, \\(cor(X_1,Y)\\). We typically use \\(R^2\\) to examine goodness-of-fit, while Pearson’s correlation shows how \\(X_1\\) and \\(Y\\) move together. "],["permutation-test-for-slope.html", "Chapter 11 Permutation Test for Slope 11.1 Usage 11.2 How it Works", " Chapter 11 Permutation Test for Slope 11.1 Usage The permutation test for the slope helps to determine if there is a significant linear relationship between \\(X\\) and \\(Y\\), when we can’t assume normally distributed error terms. 11.1.1 Assumptions We have a random sample of paired measurements \\((X_i,Y_i)\\). 2.The relationship between \\(X_i\\) and \\(Y_i\\) is linear. The error terms \\(\\varepsilon_i\\) are independent and identically distributed (iid) about zero. However, we no longer assume a Normal distribution. 11.2 How it Works Like many of the previous permutatation tests, we create a p-value that reflects the probability Under \\(H_0\\), there is no relation b/w \\(X\\) and \\(Y\\), so any of the observed \\(Y_i\\)’s could have come from any of the \\(X_i\\)’s. Given \\(n\\) observations, there are \\(n!\\) ways to reorder \\(Y_i\\)’s, because we fix \\(X_i\\)’s and shuffle the \\(Y_i\\)’s 11.2.1 p-value Reshuffle the \\(Y_i\\)’s to get new pairs \\((X_i, Y_i*)\\) Calculate \\(\\hat{\\beta}_1^*\\) for the permuted sample Repeat steps 1 and 2 each of \\(n!\\) times to generate all possible permutations p-value is the fraction of \\(\\hat{\\beta}_1^*\\) as or more extreme than observed: \\[ \\begin{array}{l} p_{\\text {lower tail}}=\\frac{\\# \\hat{\\beta}_{1}^{*} \\leq \\hat{\\beta}_{1, o b s}}{n !} \\\\ p_{\\text {upper tail}}=\\frac{\\# \\hat{\\beta}_{1}^{*} \\geq \\hat{\\beta}_{1, o b s}}{n !} \\\\ p_{\\text {two sided}}=\\frac{\\# |\\hat{\\beta}_{1}^{*}| \\geq |\\hat{\\beta}_{1, o b s}|}{n !} \\end{array} \\] "],["spearmans-rank-correlation.html", "Chapter 12 Spearman’s Rank Correlation 12.1 Usage", " Chapter 12 Spearman’s Rank Correlation 12.1 Usage Spearman’s rank correlation (\\(r_s\\)) calculates the correlation between ranked observations. \\(r\\) calculates the correlation between the pairs \\((X_i,Y_i)\\) \\(r_s\\) calculates the correlation between the pairs \\((~R(X_i),R(Y_i)~)\\) Assumptions We must have independent paired observations. We can’t measure the association of dependent data (i.e. time series). 12.1.1 How it Works By comparing ranks of \\(X_i\\)’s to ranks of \\(Y_i\\)’s, we can see the extent to which \\(Y\\) increases or decreases with \\(X\\). The p-value follows a similar permutation pattern as before. We can reshuffle the obser 12.1.2 p-value Reshuffle the \\(Y_i\\)’s to get new pairs \\((X_i,Y^*_i)\\) Calculate \\(r^*_s\\) for the permuted sample. Repeat steps 1 and 2, \\(n!\\) times to generate all possible permutations. p-value is the fraction of \\({r}_s^*\\)’s as or more extreme than observed: Formal Hypothesis Test Hypothesis Test: \\[ \\begin{aligned} H_0 &amp;: \\rho_s=0 \\\\ H_a &amp;: \\rho_s \\geq 0, \\rho_s \\leq 0, \\text{ or } \\rho_s \\neq 0 \\end{aligned} \\] Note: \\(\\rho_s\\) refers to the population measure, while \\(r_s\\) refers to the observed correlation measure. P-Value: \\[ \\begin{array}{l} p_{\\text {lower tail}}=\\frac{\\# r_{s}^{*} \\leq r_{s, obs}}{n !} \\\\ p_{\\text {upper tail}}=\\frac{\\# r_{s}^{*} \\geq r_{s, obs}}{n !} \\\\ p_{\\text {two sided}}=\\frac{\\# |r_{s}^{*}| \\geq |r_{s, obs}|}{n !} \\end{array} \\] "],["kendalls-tau.html", "Chapter 13 Kendall’s Tau 13.1 How it Works 13.2 Limitations", " Chapter 13 Kendall’s Tau Kendall’s Tau \\(\\tau\\) is a measure of association between X and Y based on concordance. 13.1 How it Works Concordance suggests that observations are consistent with one another; that they are positively associated. More formally, we say that a pair of points \\(\\left(X_{i}, Y_{i}\\right)\\) and \\(\\left(X_{j}, Y_{j}\\right)\\) are: Concordant if \\(X_{i}&lt;X_{j} \\Rightarrow Y_{i}&lt;Y_{j}\\), or \\(\\left(X_{i}-X_{j}\\right)\\left(Y_{i}-Y_{j}\\right)&gt;0\\) If pairs are more likely to be concordant \\(\\implies\\) positive association Discordant if \\(X_{i}&lt;X_{j} \\Rightarrow Y_{i}&gt;Y_{j}\\), or \\(\\left(X_{i}-X_{j}\\right)\\left(Y_{i}-Y_{j}\\right)&lt;0\\) If pairs are more likely to be discordant \\(\\implies\\) negative association We define Kendall’s Tau as: \\[ \\begin{aligned} r_{\\tau} &amp;= \\frac{(\\text{# concordant pairs})-(\\text{# discordant pairs})}{\\binom{n}{2}} \\end{aligned} \\] Why use \\(\\binom{n}{2}\\) instead of \\(n\\)? \\[ \\begin{array}{l} p_{\\text {lower tail}}=\\frac{\\# r_{\\tau}^{*} \\leq r_{\\tau, obs}}{n !} \\\\ p_{\\text {upper tail}}=\\frac{\\# r_{\\tau}^{*} \\geq r_{\\tau, obs}}{n !} \\\\ p_{\\text {two sided}}=\\frac{\\# |r_{\\tau}^{*}| \\geq |r_{\\tau, obs}|}{n !} \\end{array} \\] 13.2 Limitations Both look for a monotonic trend only, neither can detect a parabolic trend (\\(r_s=r_{\\tau}=0\\)) We must have independent paired observations (can’t use time series b/c dependence) "]]
