[["summary.html", "Summary Background Notation Diagnostics", " Summary Goal: We’ve got two samples, and want to see if there’s the significance difference in means. Background Comparing differences in means is one of the most commonly used procedures in statistics. Take the following use case: Does Product A have better ratings than than Product B? Sure, we could compare averages (i.e. Product A’s mean is \\(4.83\\), better than Product B’s \\(4.79\\)), but it fails to answer the question: Are the population ratings for A better than B? We only have data from a sample of reviews, so we’ve got to somehow estimate the population differences in ratings. Any student of introductory statistics knows the remedy: take into account the spread of the data using a two-sample t-test. But the t-test makes a few strong assumptions about the data, mainly the assumption of normality. This chapter describes alternative tests we can use instead. Notation Say you’re expecting population 1 to be lower than population 2. You’ll use a lower-tail test, and use a set of hypotheses like so: \\[ H_0: \\mu_1 = \\mu_2 \\\\ H_a: \\mu_1 \\leq \\mu_2 \\] In this section, we don’t necessarily know the distribution of our data, but we’re testing that the distributions, not necessarily means, are equal. As such, our hypotheses will look something like: \\[ H_0: F_1(x) = F_2(x) \\\\ H_a: F_1(x) \\geq F_2(x) \\\\ \\text{ with a strict inequality for at least one }x \\] \\(F(x)\\) here refers to the empirical CDF, or the probability of seeing a value less than or below \\(x\\). If the location of population 1 is below population 2, we’d expect that at a certain point \\(x\\), the probability of seeing a value at or below \\(x\\) should be higher for population 1 than it is for population 2. Here’s a visual: From the first plot we see \\(\\mu_1 &lt; \\mu_2\\), and from the second, \\(F_1(x) \\geq F_2(x)\\). Diagnostics A big assumption parametric tests take is that the data is normally distributed. There’s a few approaches we can take to validate this. First is the QQ plot: We could also use a more rigorous test of normality: But its almost always just worth checking the distribution regardless: "],["choosing-a-test.html", "Choosing a Test", " Choosing a Test Are your samples independent? Are your population distributions normal, or, are there more than 40 observations in each? If so, choose the Parametric t-Test. Do you care about the magnitude of the difference? Consider the Permutation Test. Check the distribution and number of outliers. If the data is skewed, use the median variant, and if there are outliers on both tails, consider using trimmed means. If you’re instead just looking to determine if one sample is greater than the other, use the Wilcoxon Rank-Sum, particularly with skewed distributions or heavy outliers. If we’re interested in generating a confidence interval for the difference, we can use the Mann-Whitney test. It works similarly to the Wilcoxon Rank-Sum, and results in the same p-value. "],["two-sample-t-test.html", "Chapter 1 Two Sample t-Test 1.1 How It Works 1.2 Code", " Chapter 1 Two Sample t-Test The two-sample t-test is by far the most powerful test when the assumptions are met for comparing means between two samples. Assumptions: Random sample from each population Both samples are independent Both population distributions are normal Both population variances are equal Note: By the Central Limit Theorem, we can assume that the sample means will start looking normal at large sample sizes (\\(n \\geq 40\\)). 1.1 How It Works Do we know the population variance \\(\\sigma^2\\)? If so, we’ll use the \\(z\\) distribution: \\(z\\sim N(0,1)\\) Is the population variance \\(\\sigma^2\\) unknown? If not, we’ll then use the \\(t\\) distribution, \\(t\\sim t(df)\\), where \\(df\\) is the minimum of the two sample sizes - 1. 1.2 Code R Python library(stats) sample1 &lt;- c(1.1, 2.1, 4.2, 3.2, 1.7, 2.2, 2.7) sample2 &lt;- c(3.9, 2.9, 3.8, 1.8, 3.3, 2.8, 2.3) t.test(x=sample1, y=sample2, alternative=&#39;two.sided&#39;, paired=F) from scipi.stats import ttest_ind sample1 = [1.1, 2.1, 4.2, 3.2, 1.7, 2.2, 2.7] sample2 = [3.9, 2.9, 3.8, 1.8, 3.3, 2.8, 2.3] ttest_ind(a=sample1, b=sample2) "],["two-sample-permutation.html", "Chapter 2 Permutation Test 2.1 How It Works 2.2 Code 2.3 Variants", " Chapter 2 Permutation Test Use the permutation test if the normality assumption is violated, and you’re interested in quantifying the difference in some location parameter: mean, trimmed mean, or median. This works well for smaller sample sizes. Assumptions: Random sample from each population Both are sampled independently Both population distributions are continuous (not categorical / discrete) We no longer need the assumption of normality, nor equal variances 2.1 How It Works We’ll use \\(D_{obs}\\) to represent the difference in means that we observe between our samples. Under the null hypothesis, we’d expect that there is no difference in means. In other words, we could randomly switch around the values across the samples, and each time expect to get a test statistic \\(D^*\\) close to \\(D_{obs}\\). If the null hypothesis is false, and the difference we observe can’t be explained by random chance, we’d see that only a few \\(D^*\\)’s are more extreme than \\(D_{obs}\\). More on those permutations: If sample 1 has \\(m\\) observations, and sample 2 has \\(n\\) observations, there are \\(\\binom {m+n}{m} = \\frac {(m+n)!}{m!n!}\\) permutations when we pool together our observations and reassign them to a group. We can calculate a test statistic \\(D^*\\) for each one of these permutations. So, our p-value is then just the fraction of permutations that have a test statistic \\(D^*\\) as or more extreme than what was observed \\(D_{obs}\\). Formal Definitions For a double sided test: \\[ H_0: F_1(x) = F_2(x) \\\\ H_a: F_1(x) \\neq F_2(x) \\\\ ~ \\\\ p\\text{-value}_{two\\ sided} = \\frac{\\text{# of |D&#39;s|}~\\geq~|D_{obs}|}{\\binom {m+n}{m}} \\] For an upper tail test: \\[ H_0: F_1(x) = F_2(x) \\\\ H_a: F_1(x) \\leq F_2(x) \\\\ ~ \\\\ p\\text{-value}_{upper} = \\frac{\\text{# of }D\\geq D_{obs}}{\\binom {m+n}{m}} \\] For a lower tail test: \\[ H_0: F_1(x) = F_2(x) \\\\ H_a: F_1(x) \\geq F_2(x) \\\\ ~ \\\\ p\\text{-value}_{lower} = \\frac{\\text{# of }D\\leq D_{obs}}{\\binom {m+n}{m}} \\] Interpretation: Given a p-value of 0.06, there is a 6% chance of observing a difference as extreme as we did under the hypothesis that these samples come from populations with the same distribution. 2.2 Code R Python source(&#39;https://raw.githubusercontent.com/ishaandey/nonparametric/main/helper.R&#39;) sample1 &lt;- c(37, 49, 55, 57) sample2 &lt;- c(23, 31, 39, 46) permutation_test(sample1, sample2, method=&#39;mean&#39;) # Could also use method=&#39;median&#39; # Function defined at &#39;https://raw.githubusercontent.com/ishaandey/nonparametric/main/helper.py&#39; sample1 = [37, 49, 55, 57] sample2 = [23, 31, 39, 46] permutation_test(sample1, sample2, method=&#39;mean&#39;) 2.3 Variants Instead of difference in means, we could use either (1) sums, (2) trimmed means, or (3) medians: Mean/Sum: Use when pop. dist. is short-tailed (normal looking) Trimmed Mean: Use when pop. dist. is symmetric but heavy-tailed (some unusually extreme observations are likely) Median: Use when population distribution is skewed "],["wilcoxon-rank-sum.html", "Chapter 3 Wilcoxon Rank-Sum 3.1 How It Works 3.2 Code", " Chapter 3 Wilcoxon Rank-Sum Wilcoxon Rank-Sum is great for testing with low sample sizes and outliers, since it uses the rank of the observation as opposed to the value itself. There’s only one assumption, that both population distribution are continuous (not categorical / discrete) 3.1 How It Works The goal here is to use ranks, as opposed to the actual values, to identify differences in location between the two parameters. Why? Ranks are much more resistant to outliers, since a singly high observation is now just ranked at the max, doesn’t matter how far above in absolute value it is. We can pool the observations and compare the ranks that were assigned to sample 1 against those assigned to sample 2. We can compare the average rank of sample 1 against sample 2, and if it’s lower by a particular margin, we can conclude that the values of sample 1 are below sample 2. As an example: Sample 1 31 32 33 47 Sample 2 46 48 49 51 We’ll first calculate \\(W_{obs}\\), our test statistic. To do so, we’ll first pool both samples together and rank them, assigning a value of 1 to the smallest observation, and m+n to the largest (since there are now \\(m+n\\) observations in the pooled group). Values 31 32 33 47 46 48 49 51 Ranks 1 2 3 5 4 6 7 8 \\(W_{obs}\\) is simply the sum of ranks of sample 1 observations: \\(W_{obs}=\\) 1+2+3+5 \\(=11\\). Why use this instead of the mean of the ranks between the two? Turns out, the sum of ranks of one sample is a linear function of the mean, so there’s a 1:1 correspondence between the two. The sum of ranks is just more consistent and easy to calculate. Under the null hypothesis, we’d expect our observations to have no difference in ranks. So, if we were to randomly switch around (permute) the observations across the samples, we’d expect our observed test statistic \\(W_{obs}\\) to not be anything unusual, according to \\(H_0\\). In otherwords, random chance could have just as easily produced \\(W_{obs}\\) as the treatment we gave. Let’s make that idea a little more quantitative: If sample 1 has \\(m\\) observations, and sample 2 has \\(n\\) observations, there are \\(\\binom {m+n}{m} = \\frac {(m+n)!}{m!n!}\\) permutations when we pool together our observations and reassign them to a group. We can calculate a test statistic \\(W^*\\) for each one of these permutations. Our p-value is then just the fraction of permutations that have a test statistic \\(W\\) as or more extreme than what was observed \\(W_{obs}\\): Going back to our example, Here are P-value Formula \\(p-val_{two\\ sided} = \\frac{\\text{# of W&#39;s more extreme than } W_{obs} \\text{ across both tails}}{\\binom {m+n}{m}}\\) \\(p-val_{lower} = \\frac{\\text{# of }W\\geq W_{obs}}{\\binom {m+n}{m}}\\) \\(p-val_{upper} = \\frac{\\text{# of }W\\leq W_{obs}}{\\binom {m+n}{m}}\\) In our example, there are \\(\\binom {8}{4}=70\\) possible ways we could’ve obtained the four observations in sample 1 from a total of eight values. We could lookup the significance from a table, or read off the p-value from the test output Interpretation: Given a p-value of 0.028, there is a 2.8% chance of observing a difference as extreme as we did under the hypothesis that these samples come from populations with the same distribution. Because it’s less than our confidence threshold \\(\\alpha\\) of \\(0.05\\), we reject the null hypothesis that \\(F_1(x) = F_2(x)\\), and conclude that the location of population 1 is lower than population 2. 3.2 Code R Python library(stats) sample1 &lt;- c(31, 32, 33, 47) sample2 &lt;- c(46, 48, 49, 51) wilcox.test(sample1, sample2, alternative=&quot;less&quot;) ## ## Wilcoxon rank sum exact test ## ## data: sample1 and sample2 ## W = 1, p-value = 0.02857 ## alternative hypothesis: true location shift is less than 0 from scipy.stats import ranksums sample1 = [31, 32, 33, 47] sample2 = [46, 48, 49, 51] ranksums(sample1, sample2, alternative=&#39;less&#39;) "],["mann-whitney.html", "Chapter 4 Mann-Whitney 4.1 How It Works 4.2 Code 4.3 Note", " Chapter 4 Mann-Whitney Assumptions Both pop. dist are continuous (not categorical / discrete) 4.1 How It Works In a sample of \\(m\\) observations in sample \\(X\\), and \\(n\\) observations in sample \\(Y\\), we want to focus on each possible pair of observations. The test statistic is quite simply \\(U = \\text{# pairs for which } X_i &lt; Y_j\\). The minimum \\(U\\) can be is \\(0\\), while the max is every possible pair, or \\(m*n\\). For example, let’s say we have two samples, and want to see if sample 1 has a lower location than sample 2. Here’s our raw data: Sample 1 31 33 46 40 Sample 2 39 49 55 57 We then look at every possible combination of the two samples and compare the values. We check if the values of the first sample (the columns) are greater than the values of the second sample (rows): Is Sample 1 &gt; Sample 2? 31 33 40 46 39 N N Y Y 49 N N N N 55 N N N N 57 N N N N Our test statistic \\(U_{obs}\\) is the number of Y’s we found in the matrix, \\(=2\\). Under the null hypothesis, we’d expect our test statistic not to be that extreme. That is, if we were to randomly permute our values under different label, we’d need to see that our observed test statistic is far more extreme that the rest of the U statistics before concluding that the null hypothesis doesn’t apply. Formal Hypothesis Tests Formal Hypothesis Test Hypothesis Test: \\[ \\begin{aligned} H_0 &amp;: F_1(x) = F_2(x) \\\\ H_a &amp;: F_1(x) \\geq F_2(x) \\\\ &amp; \\text{with a strict inequality for at least one }x \\end{aligned} \\] P-Value: \\[ \\begin{aligned} p\\text{-value}_{two\\ sided} &amp;= \\frac{\\text{# of U&#39;s farther from } \\frac{mn}{2}}{\\binom {m+n}{m}} \\\\ p\\text{-value}_{lower} &amp;= \\frac{\\text{# of }U\\geq U_{obs}}{\\binom {m+n}{m}} \\\\ p\\text{-value}_{upper} &amp;= \\frac{\\text{# of }U\\leq U_{obs}}{\\binom {m+n}{m}} \\end{aligned} \\] 4.2 Code #hello there! 4.3 Note The Wilcoxon \\(W\\) is linearly related to Mann Whitney \\(U\\), and results in the same p-value. Proof \\[ \\begin{array}{l} W_{2} \\\\ =\\sum_{j=1}^{n} R\\left(Y_{j}\\right) \\\\ =R\\left(Y_{1}\\right)+R\\left(Y_{2}\\right)+\\cdots+R\\left(Y_{n}\\right) \\\\ =\\left[1+\\left(\\text {number of } X^{\\prime} s \\leq Y_{1}\\right)\\right]+\\left[2+\\left(\\text {number of } X^{\\prime} s \\leq Y_{2}\\right)\\right]+\\cdots \\\\ =[1+\\cdots+n]+\\left[\\left(\\text {number of } X^{\\prime} s \\leq Y_{1}\\right)+\\cdots+\\left(\\text {number of } X^{\\prime} s \\leq Y_{n}\\right)\\right] \\\\ =[1+\\cdots+n]+U \\\\ =\\frac{n(n+1)}{2}+U \\end{array} \\] "]]
