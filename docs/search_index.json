[["introduction-2.html", "Introduction Background", " Introduction Goal: How can we determine if a treatment changes an outcome for each unit? Background Take the following question: Does a topical treatment lower a patient’s rating of pain? Suppose we have before and after ratings for each user. Since each user may have a different level of pain going in, taking the overall difference of means may not show any significant change. Instead, we can look at differences per-patient, and see what that looks like. As before, the parametric solution (paired t-test) makes a few strong assumptions about the data, mainly the assumption of normality. This chapter describes alternative tests we can use instead. "],["cheatsheet.html", "Cheatsheet", " Cheatsheet Look at distribution of differences: If light-tailed and symmetric \\(\\implies\\) Use paired permutation test If skewed or heavy tailed \\(\\implies\\) Use wilcoxon signed-rank test Sign test also is efficient, but generally has low power for smaller \\(n\\) If normal looking \\(\\implies\\) Use sign test "],["paired-t-test.html", "Chapter 1 Paired t-Test 1.1 How It Works 1.2 Code", " Chapter 1 Paired t-Test The paired t-test is by far the most powerful test when the assumptions are met for comparing means between two samples. Here are the assumptions: Paired observations are a random sample (independent) from population of all possible pairs The differences are normally distributed Note: By the Central Limit Theorem, we can assume that the sample means will start looking normal at large sample sizes (\\(n \\geq 40\\)). 1.1 How It Works Paired t-test statistic: \\[ t=\\frac{\\bar{x}_{d}}{S_{d} / \\sqrt{n_{d}}} \\sim t\\left(n_{d}-1\\right) \\] \\(\\bar{x}_{d}\\) is the sample mean difference. \\(s_{d}\\) is the sample standard deviation of the differences. \\(n_{d}\\) is the number of pairs. We’re trying to test the hypothesis: \\[ \\begin{aligned} H_0&amp;: \\mu_d = 0 \\\\ H_a&amp;: \\mu_d &gt; 0,~&lt;0,~\\text{or}~\\neq 0 \\end{aligned} \\] 1.2 Code 1.2.1 R library(stats) samp.before &lt;- c(1.1, 2.1, 4.2, 3.2, 1.7, 2.2, 2.7) samp.after &lt;- c(3.9, 2.9, 3.8, 1.8, 3.3, 2.8, 2.3) t.test(x=samp.before, y=samp.after, alternative=&#39;two.sided&#39;, paired=T) 1.2.2 Python from scipi.stats import ttest_rel samp_before = [1.1, 2.1, 4.2, 3.2, 1.7, 2.2, 2.7] samp_after = [3.9, 2.9, 3.8, 1.8, 3.3, 2.8, 2.3] ttest_rel(samp_before, samp_after) "],["paired-permutation-test.html", "Chapter 2 Paired Permutation Test 2.1 How It Works 2.2 Code", " Chapter 2 Paired Permutation Test We can use the permutation test if we violate the normality assumption of the paired t-test. However, as we’re still using an average of differences, our test statistic is still subject to outliers. 2.1 How It Works By convention, differences are defined as \\(\\text{treatment} - \\text{control}\\), or \\(\\text{after} - \\text{before}\\). Pre 9.1 6.2 4.2 5.9 Post 7.3 4.8 4.1 4.7 Diff -1.8 -1.4 -0.1 -1.2 We’ll define our test statistic \\(D_{obs}\\) as the average of differences: \\[ \\bar{D}=\\frac{1}{n}\\sum_{i=1}^{n} D_{i} \\] With that formula, we’ll get \\(D_{obs}=\\) -1.125. How likely is it that we see \\(D_{obs}=\\)-1.125 by random chance? Under the null hypothesis, we’d expect that if we were to randomly switch around (permute) the observations within the pair, we’d see the same test statistic. Since we have \\(n\\) pairs, there are \\(2^n=16\\) possible arrangements where we swap around the values across the treatment and control group. For each permutation, we’ll find \\(\\bar{D}^*\\), the mean of differences for that particular permutation. More visually: Obs 1 Obs 2 Obs 3 Obs 4 Obs 5 before 31 38 46 54 43 after 39 49 55 57 44 can be permuted within pairs, with one permutation looking like: Obs * Obs 2 Obs * Obs 4 Obs 5 before* 39 38 55 54 43 after* 31 49 46 57 44 but not across pairs, because we’re using a matched pairs design. So, our p-value is then just the fraction of permutations that have a test statistic \\(D\\) as or more extreme than what was observed \\(D_{obs}\\). Formal Definitions 2.2 Code R Python #add code #add code 2 "],["wilcoxon-signed-rank-test.html", "Chapter 3 Wilcoxon Signed-Rank Test 3.1 Usage 3.2 Procedure", " Chapter 3 Wilcoxon Signed-Rank Test 3.1 Usage Signed Ranks are a method of ranking matched pairs data while accounting for the positive or negative nature of the differences. In other words, we can take into account if the treatment or control was higher, without caring about how much higher it was. 3.2 Procedure For each pair of observations, we’ll take the difference as after - before. We’ll then rank these differences by absolute value, smallest to largest, but retain the sign of the rank from the actual difference. Our test statistic, \\(SR_+\\), is the sum of positive signed ranks. More visually: Obs 1 Obs 2 Obs 3 Obs 4 Obs 5 before 31 38 46 54 45 after 39 49 55 57 43 becomes Obs 1 Obs 2 Obs 1 Obs 4 Obs 5 before 31 38 46 54 45 after 39 49 55 57 43 diff 8 11 9 3 -2 sign.rank 3 5 4 2 -1 Here the the test statistic is found as \\(SR_+ = 3+5+4+2 \\implies SR_+=14\\). In order to find the p-value, we want to ask how likely is it that we found this test statistic just by random chance, assuming the null to be true? In other words, we’ll find every possible permutation of the sign-ranks, and calculate \\(SR_+^*\\) for each one. The p-value is just the fraction of ranks sums as or more extreme than what we observed. P-value Formula \\[ \\begin{aligned} P_{\\text {upper tail}} &amp;=\\frac{\\text {number of } SR_+ \\text{&#39;s} \\geq SR_+}{2^{n}} \\\\ P_{\\text {lower tail}} &amp;=\\frac{\\text {number of } SR_+ \\text{&#39;s} \\leq SR_+}{2^{n}} \\end{aligned} \\] A brief comment on ties: Details Note: When we have multiple pairs with the same difference, we can apply the average rank to the tied observations. Note: If the difference between a pair is 0, we either omit them from the sample, or ignore them (as they don’t count towards \\(SR_+\\)). You’d typically keep them since it gives similar results, and if there many zeros, you’d lose plenty of power from the lower sample size. 3.2 R library(stats) pre &lt;- c(1180, 1210, 1300, 1080, 1120, 1240, 1360, 980) post &lt;- c(1230, 1280, 1310, 1140, 1150, 1200, 1340, 1100) diff &lt;- post-pre wilcox.test(diff, alternative=&quot;greater&quot;) 3.2 Python # under construction "],["sign-test.html", "Chapter 4 Sign Test 4.1 Usage 4.2 Procedure 4.3 Code", " Chapter 4 Sign Test 4.1 Usage 4.2 Procedure 4.2.1 Hypothesis \\(H_{o}: F(x)=1-F(-x)\\) \\(H_{a}: F(x) \\leq 1-F(-x)\\) or \\(H_{a}: F(x) \\geq 1-F(-x)\\) 4.2.2 Test statistic \\(\\mathrm{SN}_{+}\\) is the number of observations greater than 0 If \\(\\mathrm{H}_{\\mathrm{o}}\\) is true, then the distribution of \\(\\mathrm{SN}_{+}\\) is: \\(SN_+ \\sim \\text{Binom}(n, 0.5)\\) or \\(SN_+ \\sim N(0.5 n, \\sqrt{0.25 n})\\) for large enough samples 4.2.3 p value \\[ P_{\\text {upper tail}}=P(SN_+ \\geq SN_{+,obs})\\\\ P_{\\text {lower tail}}=P(SN_+ \\leq SN_{+,obs}) \\] 4.3 Code 4.3 R library(stats) before &lt;- c(1180, 1210, 1300, 1080, 1120, 1240, 1360, 980) after &lt;- c(1230, 1280, 1310, 1140, 1150, 1200, 1340, 1100) diff &lt;- after - before n &lt;- length(diff) SN &lt;- length(diff[diff &gt; 0]) 1 - pbinom(SN, n, 0.5) 4.3 Python # under construction "]]
